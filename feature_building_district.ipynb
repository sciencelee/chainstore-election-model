{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all US Congressional District information\n",
    "\n",
    "## FIX ME\n",
    "All of my data is linked by the FIPS identifying code for each county.\n",
    "We will be using the FIPS data for plotting and id of county bounds and as a defacto index across all datasets.\n",
    "\n",
    "We will also be using the county name for additional identification and the census tract area for each county. The census area is the surveyed area for census purposes (where people live) within the bounds of the county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from myfunctions import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ONLY RUN THIS ONCE!!!!\n",
    "\n",
    "# shape_url = 'https://theunitedstates.io/districts/cds/2016/'\n",
    "\n",
    "# ## NY-30/shape.geojson\n",
    "\n",
    "# states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "#           \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "#           \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "#           \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "#           \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "\n",
    "# # for state in states:\n",
    "# #     page = requests.get(URL)\n",
    "\n",
    "# #     shape_urls = [x for x in range(1, 53)]\n",
    "\n",
    "# all_shape_urls = []\n",
    "\n",
    "# for state in states:\n",
    "#     flag = 0\n",
    "#     print(state)\n",
    "#     for i in range(0, 54):\n",
    "#         url = shape_url + state + '-' + str(i) + '/shape.geojson'\n",
    "#         r = requests.get(url)\n",
    "#         if r.status_code > 200 and flag > 0:\n",
    "#             flag += 1\n",
    "#             break\n",
    "#         elif r.status_code > 200:\n",
    "#             continue\n",
    "#         else:\n",
    "#             all_shape_urls.append(url)\n",
    "            \n",
    "# all_shape_urls\n",
    "\n",
    "\n",
    "# all_geojsons = []\n",
    "\n",
    "# import json\n",
    "\n",
    "# for url in all_shape_urls:\n",
    "#     resp = requests.get(url=url)\n",
    "#     data = resp.json() # Check the JSON Response Content documentation below\n",
    "#     all_geojsons.append(data)\n",
    "    \n",
    "# # Dump my new info into a pickle file.\n",
    "# # now we have all of the geojson pages in a single pickle file (list object)\n",
    "\n",
    "# with open('district_geojson.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_geojsons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_geojsons = pickle.load( open( \"datasets/district_geojson.pkl\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I ran the above cell, I can just load the pickled data.\n",
    "#all_geojsons = pickle.load( open( \"datasets/district_geojson.pkl\", \"rb\" ) )\n",
    "#type(all_geojsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_geojsons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-75522a3f6e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Let's start by making a dataframe that holds every district\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmy_district\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_geojsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmy_district\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_geojsons' is not defined"
     ]
    }
   ],
   "source": [
    "# Now build a df out of the geojsons.\n",
    "# Each row will hold district and polygon info\n",
    "\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon, shape, GeometryCollection, asShape\n",
    "\n",
    "\n",
    "# Let's start by making a dataframe that holds every district\n",
    "    \n",
    "my_district = all_geojsons[0]\n",
    "my_district.keys()\n",
    "\n",
    "\n",
    "# make a poly shape for each district\n",
    "polygons = [asShape(x['geometry']) for x in all_geojsons]\n",
    "\n",
    "\n",
    "district_names = [x['properties']['Code'] for x in all_geojsons]\n",
    "df = pd.DataFrame(district_names, columns=['district'])\n",
    "\n",
    "df['polygon'] = polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geojson\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE A DISTRICT MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_geojsons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d326be0e497a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_geojsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     folium.GeoJson(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_geojsons' is not defined"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "center= (39.8283, -98.5795, )\n",
    "\n",
    "m = folium.Map(location=center, zoom_start=5)\n",
    "\n",
    "\n",
    "for html in all_geojsons[:]:\n",
    "\n",
    "    folium.GeoJson(\n",
    "        html,\n",
    "        name='geojson',\n",
    "        control=True,\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "m.save('districts.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I will be making this map in folium instead of plotly because of some problems I ran into with format of the geojson data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def get_district(point, dist_df):\n",
    "    for i in range(len(dist_df)):\n",
    "        poly = dist_df.iloc[i]['polygon']\n",
    "        if poly.contains(point):\n",
    "            return dist_df.iloc[i]['district']\n",
    "        \n",
    "        \n",
    "def get_stores_by_dist(store_df, dist_df):\n",
    "    found = store_df['points'].apply(get_district, args=[dist_df])\n",
    "    return found.value_counts()\n",
    "\n",
    "\n",
    "def return_count(district, wf_counts):\n",
    "    try:\n",
    "        return wf_counts[district]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-185ccfe6fe14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df = pd.read_pickle('datasets/dist_store_df.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'datasets/dist_store_df.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'context'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "# Open the file from pickle to add to the df OR use it for rest of notebook\n",
    "#df = pd.read_pickle('datasets/dist_store_df.pkl')\n",
    "\n",
    "df = pickle.load( open( 'datasets/dist_store_df.pkl', \"rb\" ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "# For each store...\n",
    "# 1) import\n",
    "# 2) get a list of discticts for each store location\n",
    "# 3) use the list of district names to add a col to your df\n",
    "\n",
    "\n",
    "# WHOLE FOODS \n",
    "# #1\n",
    "# wf_df = pd.read_csv('datasets/wholefoods.csv', encoding='latin')\n",
    "# # wf_df.to_csv('datasets/wholefoods.csv')  # fixing the encoding issue\n",
    "# wf_df['longlat'] = list(zip(wf_df['long'], wf_df['lat']))\n",
    "# wf_df['points'] = wf_df['longlat'].apply(make_point)\n",
    "\n",
    "# #2\n",
    "# wf_counts = get_stores_by_dist(wf_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['WholeFoods'] = df.district.apply(return_count, args=[wf_counts])\n",
    "\n",
    "\n",
    "\n",
    "# # TRACTOR SUPPLY\n",
    "# ts_df = pd.read_csv('datasets/tractorsupply.csv')\n",
    "# ts_df.info()\n",
    "# ts_df = add_longlat(ts_df)\n",
    "# ts_df.describe()\n",
    "# #2\n",
    "# ts_counts = get_stores_by_dist(ts_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['TractorSupply'] = df.district.apply(return_count, args=[ts_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # STARBUCKS\n",
    "# sb_df = pd.read_csv('datasets/starbucks.csv')\n",
    "# sb_df = sb_df.rename(columns={'Longitude': \"long\", \"Latitude\":'lat'})\n",
    "# sb_df = sb_df[sb_df['Country']=='US']\n",
    "# sb_df = add_longlat(sb_df)\n",
    "\n",
    "# #2\n",
    "# sb_counts = get_stores_by_dist(sb_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['Starbucks'] = df.district.apply(return_count, args=[sb_counts])\n",
    "\n",
    "\n",
    "\n",
    "# ACADEMY SPORTS\n",
    "# as_df = pd.read_csv('datasets/academy.csv')\n",
    "# as_df = add_longlat(as_df)\n",
    "# as_df.describe()\n",
    "\n",
    "# #2\n",
    "# as_counts = get_stores_by_dist(as_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['AcademySports'] = df.district.apply(return_count, args=[as_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # DICK'S SPORTS\n",
    "# ds_df = pd.read_csv('datasets/dicks.csv')\n",
    "# ds_df = add_longlat(ds_df)\n",
    "# ds_df.describe()\n",
    "\n",
    "# #2\n",
    "# ds_counts = get_stores_by_dist(ds_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['DicksSports'] = df.district.apply(return_count, args=[ds_counts])\n",
    "\n",
    "\n",
    "\n",
    "# # BASS PRO SHOPS\n",
    "# bp_df = pd.read_csv('datasets/basspro.csv')\n",
    "# bp_df = add_longlat(bp_df)\n",
    "# bp_df.describe()\n",
    "\n",
    "# #2\n",
    "# bp_counts = get_stores_by_dist(bp_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['BassPro'] = df.district.apply(return_count, args=[bp_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # DOLLAR TREE\n",
    "# dt_df = pd.read_csv('datasets/dollartree.csv')\n",
    "# dt_df = add_longlat(dt_df)\n",
    "# dt_df.describe()\n",
    "\n",
    "# #2\n",
    "# dt_counts = get_stores_by_dist(dt_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['DollarTree'] = df.district.apply(return_count, args=[dt_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # GRAINGER STORE\n",
    "# grg_df = pd.read_csv('datasets/grainger.csv')\n",
    "# grg_df.info()\n",
    "# grg_df = add_longlat(grg_df)  # this one had a bunch of garbage and extra commas\n",
    "# grg_df.describe()\n",
    "\n",
    "# #2\n",
    "# grg_counts = get_stores_by_dist(grg_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['Grainger'] = df.district.apply(return_count, args=[grg_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # CHICK FIL A\n",
    "# cf_df = pd.read_csv('datasets/chickfila.csv', encoding='latin')\n",
    "# cf_df.info()\n",
    "# #cf_df.to_csv('datasets/chickfila.csv')\n",
    "# #time.sleep(5)\n",
    "# #cf_df = pd.read_csv('datasets/chickfila.csv', encoding='latin')\n",
    "# cf_df = add_longlat(cf_df)  # format was wrong, had to resave as utf-8\n",
    "# cf_df.describe()\n",
    "\n",
    "# #2\n",
    "# cf_counts = get_stores_by_dist(cf_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['ChickFilA'] = df.district.apply(return_count, args=[cf_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # CRACKER BARREL\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/crackerbarrel.pkl','rb') as f:\n",
    "#     cb_df = pickle.load(f)\n",
    "\n",
    "# cb_df = pd.DataFrame(cb_df)\n",
    "# cb_df.columns = ['long', 'lat']\n",
    "# cb_df = add_longlat(cb_df)\n",
    "# cb_df.info()\n",
    "\n",
    "# #2\n",
    "# cb_counts = get_stores_by_dist(cb_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['CrackerBarrel'] = df.district.apply(return_count, args=[cb_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # HARLEY DAVIDSON\n",
    "# hd_df = pd.read_csv('datasets/harley.csv', encoding='latin')\n",
    "# #dt_hd.to_csv('datasets/harley.csv')  # did this to fix encoding\n",
    "# hd_df = add_longlat(hd_df)\n",
    "# hd_df.describe()\n",
    "\n",
    "# #2\n",
    "# hd_counts = get_stores_by_dist(hd_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['HarleyDavidson'] = df.district.apply(return_count, args=[hd_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # H&M\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/hm.pkl','rb') as f:\n",
    "#     hm_df = pickle.load(f)\n",
    "\n",
    "# hm_df = pd.DataFrame(hm_df)\n",
    "# hm_df.columns = ['long', 'lat']\n",
    "# hm_df = add_longlat(hm_df)\n",
    "# hm_df.info()\n",
    "\n",
    "# #2\n",
    "# hm_counts = get_stores_by_dist(hm_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['HandM'] = df.district.apply(return_count, args=[hm_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Hobby Lobby\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/hobbylobby.pkl','rb') as f:\n",
    "#     hl_df = pickle.load(f)\n",
    "\n",
    "# hl_df = pd.DataFrame(hl_df)\n",
    "# hl_df.columns = ['long', 'lat']\n",
    "# hl_df = add_longlat(hl_df)\n",
    "# hl_df.info()\n",
    "\n",
    "# #2\n",
    "# hl_counts = get_stores_by_dist(hl_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['HobbyLobby'] = df.district.apply(return_count, args=[hl_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # LL Bean\n",
    "# ll_df = pd.read_csv('datasets/llbean.csv')\n",
    "# ll_df.info()\n",
    "# ll_df = add_longlat(ll_df)  \n",
    "# ll_df.describe()\n",
    "\n",
    "# #2\n",
    "# ll_counts = get_stores_by_dist(ll_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['LLBean'] = df.district.apply(return_count, args=[ll_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Pottery Barn\n",
    "# pb_df = pd.read_csv('datasets/potterybarn.csv')\n",
    "# pb_df.info()\n",
    "# pb_df = add_longlat(pb_df)  \n",
    "# pb_df.describe()\n",
    "\n",
    "# #2\n",
    "# pb_counts = get_stores_by_dist(pb_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['PotteryBarn'] = df.district.apply(return_count, args=[pb_counts])\n",
    "\n",
    "\n",
    "\n",
    "# # REI\n",
    "# rei_df = pd.read_csv('datasets/rei.csv')\n",
    "# rei_df.info()\n",
    "# rei_df = add_longlat(rei_df)  \n",
    "# rei_df.describe()\n",
    "\n",
    "# #2\n",
    "# rei_counts = get_stores_by_dist(rei_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['REI'] = df.district.apply(return_count, args=[rei_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Target\n",
    "# tg_df = pd.read_csv('datasets/target.csv', encoding='latin')\n",
    "# tg_df = tg_df.rename(columns={'Address.Longitude': \"long\", \"Address.Latitude\":'lat'})\n",
    "# tg_df.info()\n",
    "# #tg_df.to_csv('datasets/target.csv')  # did this to fix encoding\n",
    "# tg_df = add_longlat(tg_df)  \n",
    "# tg_df.describe()\n",
    "\n",
    "# #2\n",
    "# tg_counts = get_stores_by_dist(tg_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['Target'] = df.district.apply(return_count, args=[tg_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Trader Joe's\n",
    "# tj_df = pd.read_csv('datasets/traderjoes.csv')\n",
    "# tj_df.info()\n",
    "# tj_df = add_longlat(tj_df)  \n",
    "# tj_df.describe()\n",
    "\n",
    "# #2\n",
    "# tj_counts = get_stores_by_dist(tj_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['TraderJoes'] = df.district.apply(return_count, args=[tj_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # WalMart. \n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/walmart.pkl','rb') as f:\n",
    "#     wal_df = pickle.load(f)\n",
    "\n",
    "# wal_df = pd.DataFrame(wal_df)\n",
    "\n",
    "# wal_df\n",
    "# wal_df.columns = ['long', 'lat']\n",
    "# wal_df = add_longlat(wal_df)\n",
    "# wal_df.info()\n",
    "\n",
    "# #2\n",
    "# wal_counts = get_stores_by_dist(wal_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['Walmart'] = df.district.apply(return_count, args=[wal_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # COSTCO\n",
    "# cc_df = pd.read_csv('datasets/costco.csv', encoding='latin')\n",
    "# #cc_df = cc_df.drop(columns=['Unnamed: 0'])\n",
    "# #cc_df.to_csv('datasets/costco.csv')  # did this to fix encoding\n",
    "# cc_df.info()\n",
    "# cc_df = add_longlat(cc_df)  \n",
    "# cc_df.info()\n",
    "\n",
    "# #2\n",
    "# cc_counts = get_stores_by_dist(cc_df, df) # get the district list\n",
    "\n",
    "# #3\n",
    "# df['Costco'] = df.district.apply(return_count, args=[cc_counts])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UNCOMMENT THIS LINE TO SAVE THE DATA AGAIN\n",
    "# df.to_pickle(\"datasets/dist_store_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ea8415b8a3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 Midterm Election Data\n",
    "\n",
    "The election data is taken from https://electionlab.mit.edu/data.\n",
    "The data has results for every election, both state and national.  It includes the vote total for each candidate and the candidate's party.\n",
    "\n",
    "We will use the fips code for each county to act as an index for the more than 3000 counties contained in the dataset.  Fips codes have a two digit state id followed by a three digit county id code within the state.  States are listed in alphabetical order.  The dataset also contains information on territories (Guam, NMI, PR, etc.) but that will be filtered out to focus on the contiguous United States and Hawaii.  The burroughs of Alaska are unfortunately not part of this dataset, although after looking at the frequency of stores in Alaska, it might  not be useful to add to the model as many of these stores are not present or sparse in our 49th state.  District of Columbia is unfortunately not included.  Adding the Alaskan burroughs and DC would be a recommendation for further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in election results from csv\n",
    "df = pd.read_csv('datasets/distric_overall_2018.csv', encoding='latin') \n",
    "df.to_csv('datasets/distric_overall_2018.csv')\n",
    "\n",
    "# Eliminates territories (past Wyoming last alphabetically)\n",
    "# df = df[df['state_fips'].apply(int) < 57]\n",
    "# df['state_fips'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the election data \n",
    "I chose to go with only the US House races.  \n",
    "Senators tend to have long term support statewide (in some states) and might influence the model.\n",
    "\n",
    "Further investigation: you could look at all races cumulative to determine red or blue counties.\n",
    "Many local elections are decided cross party, unlike congressional elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all lines that are not republican or democrat\n",
    "df.party.unique() # two party data only, there are lots of them \n",
    "df = df[(df['party']=='republican') | (df['party']=='democrat')]  # df2 is two party only\n",
    "df.party.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to see if model perfoms better on national vs. local elections, so I can try both here.  \n",
    "# Might show the partisan lean better than state and local\n",
    "# Three imported lists exist (all_offices, major_offices, house and congress), see myfunctions module\n",
    "\n",
    "\n",
    "# # Put in the data you wish to include in the model.  You could use congress, major, or all (I chose house)\n",
    "df2 = df[df['office'].isin(all_offices)]  # all_offices is a list from import\n",
    "\n",
    "df_iowa = df[df['state_fips'] == 19]\n",
    "\n",
    "# This is a little fix for Iowa which is missing one congressional race if you want to look at house.\n",
    "# Data won't be perfect, but close\n",
    "df2 = pd.concat([df2, df_iowa])\n",
    "\n",
    "\n",
    "# eliminate unnecessary columns\n",
    "df2 = df2[['state', 'county', 'state_fips', 'party', 'candidatevotes', 'totalvotes', 'office']]\n",
    "\n",
    "df2.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided to just count up all of the votes for each party in every election.\n",
    "# If we chose to include local elections or major elections, this would\n",
    "# weight congressional elections the same as local, but is aimed at getting a sense of how much\n",
    "# the county leans red or blue without individual politicians affecting the categorization\n",
    "df2_grouped = df2.groupby(by=['state_fips', 'county', 'party']).sum().reset_index()\n",
    "\n",
    "# This drops out territories by chopping off everything after Wyoming\n",
    "df2_grouped = df2_grouped[df2_grouped['state_fips'].apply(int) < 57]\n",
    "df2_grouped.head()\n",
    "\n",
    "# we are left with vote totals for GOP and Dem for each county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the county data\n",
    "County naming is not standardized. There were lots of exceptions in the more than 3000 counties that had to be addressed. The data in the us_county dataset was not named the same as election data. Each .replace was a manual change to get the two datasets to conform. The election data did not have FIPS data initially included. That is unfortunate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new col named fips which is initially filled with zeroes\n",
    "df2_grouped_fips = df2_grouped.copy()\n",
    "df2_grouped_fips['fips'] = pd.Series([0 for x in range(len(df2_grouped_fips.index))])  \n",
    "\n",
    "\n",
    "# The code below ensures that county names match exactly so we can populate the fips column\n",
    "# this will make fips a key used for election, county, and population datasets\n",
    "us_county['NAME'] = us_county['NAME'].apply(lambda x: x.upper())\n",
    "us_county['NAME'] = us_county['NAME'].apply(lambda x: x.replace('.', '')\n",
    "                                                              .replace(\"''\", '')\n",
    "                                                              .replace(\"DE WITT\", 'DEWITT')\n",
    "                                                              .replace('LA SALLE', 'LASALLE')\n",
    "                                                              .replace(\"DE KALB\", 'DEKALB')\n",
    "                                                              .replace(\" CITY\", '')\n",
    "                                                              .strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2_grouped_fips['county'] = df2_grouped_fips['county'].apply(lambda x: x.upper())\n",
    "df2_grouped_fips['county'] = df2_grouped_fips['county'].apply(lambda x: x.replace('COUNTY', '')\n",
    "                                                                          .replace('.', '')\n",
    "                                                                          .replace(' CITY', '')\n",
    "                                                                          .replace('MEEER', 'MEEKER')\n",
    "                                                                          .replace('JODAVIESS', 'JO DAVIESS')\n",
    "                                                                          .replace('&', 'AND')\n",
    "                                                                          .replace('DE WITT', 'DEWITT')\n",
    "                                                                          .replace('DE KALB', 'DEKALB')\n",
    "                                                                          .replace('LAC QUI PARTE', 'LAC QUI PARLE')\n",
    "                                                                          .replace('OGLALA LAKOTA', 'OGLALA')\n",
    "                                                                          .replace('CHENAGO', 'CHENANGO')\n",
    "                                                                          .replace('LA SALLE', 'LASALLE')\n",
    "                                                                          .replace('DONA ANA', 'DOÃ‘A ANA')\n",
    "                                                                          .strip())\n",
    "\n",
    "# These are county names we dropped\n",
    "# They inclued UOCAVA (overseas votes), Oglala Lakota and some other NON-COUNTY data\n",
    "df2_grouped_fips = df2_grouped_fips.loc[~df2_grouped_fips['county'].isin(['STATE TOTALS', \n",
    "                                                                            'STATE UOCAVA', \n",
    "                                                                            'TOTAL VOTES BY CANDIDATE',\n",
    "                                                                            'TOTAL VOTES BY PARTY',\n",
    "                                                                            'KANSAS',\n",
    "                                                                            'OGLALA',\n",
    "                                                                            'FEDERAL PRECINCT',\n",
    "                                                                          \n",
    "                                                                        ])]\n",
    "\n",
    "# Force numperic values \n",
    "us_county.info()\n",
    "df2_grouped_fips.info()\n",
    "us_county[\"STATE\"] = pd.to_numeric(us_county[\"STATE\"])\n",
    "us_county[\"fips\"] = pd.to_numeric(us_county[\"fips\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this loop to go back and correct the county names between the two datasets\n",
    "errors = 0  # track the dumped counties\n",
    "\n",
    "for i in range(len(df2_grouped_fips)):\n",
    "    state = int(df2_grouped_fips.iloc[i, :]['state_fips'])  # get state number (1 to 57 numeric)\n",
    "    county = df2_grouped_fips.iloc[i, :]['county'].strip() # get county name\n",
    "    #print(us_county_df.loc[us_county_df['NAME'] == county].iloc[-1][-1])\n",
    "    try:\n",
    "        # try to associate add the approprate fips for each county\n",
    "        fip = us_county.loc[(us_county['NAME']==county) & (us_county['STATE']==state)].iloc[-1][-1]\n",
    "        df2_grouped_fips.iloc[i, -1] = fip  # BE CAREFUL HERE\n",
    "    except:\n",
    "        # if it didn't work, print it out for troubleshooting\n",
    "        print(county, state, i)\n",
    "        errors +=1\n",
    "        \n",
    "\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df2_grouped_fips.copy().reset_index()  # failed at index 414, index was missing for agg(idxmax) (Fixed)\n",
    "\n",
    "\n",
    "# get only highest (DEM or GOP)\n",
    "# This eliminates the 'loser' of each county (this is aggregate votes, not individual elections)\n",
    "df_final = df_final.iloc[df_final.groupby('fips')['candidatevotes'].agg(pd.Series.idxmax)]\n",
    "#df2_final = df2_final.iloc[df2_final.groupby('fips')['candidatevotes'].idxmax().values.ravel()]\n",
    "\n",
    "df_final.describe()\n",
    "\n",
    "\n",
    "# FINAL TALLY\n",
    "# US HAS 3141 total counties.\n",
    "\n",
    "# Missing counties\n",
    "# DC has no counties.  Not sure how to handle that\n",
    "# MISSING OGLALA LAKOTA county (Native American lands)\n",
    "# MISSING the 19 ALASKA buroughs data \n",
    "# MAY BE MISSING MORE COUNTY EQUIVALENTS\n",
    "\n",
    "# Data is missing for Iowa US Congressional race, must use all offices instead.  bummer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_final = df_final.copy()\n",
    "\n",
    "\n",
    "\n",
    "# MAKE MY FIPS COMPATIBLE WITH THE GEODATA\n",
    "df2_final['fips'] = df2_final['fips'].apply(lambda x: \"{:05}\".format(x))\n",
    "\n",
    "df2_final['blue'] = df2_final['party'].apply(lambda x: 0 if (x=='republican') else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets are stored in the /datasets folder which contains:\n",
    "- academy.csv\t\t\t\n",
    "- county_boundaries_new.json\t\n",
    "- grainger.csv\t\t\t\n",
    "- rei.csv\t\t\t\t\n",
    "- walmart.pkl\n",
    "- basspro.csv\t\t\t\n",
    "- county_fips_master.csv\t\t\n",
    "- harley.csv\t\t\t\n",
    "- starbucks.csv\t\t\t\n",
    "- wholefoods.csv\n",
    "- chickfila.csv\t\t\t\n",
    "- crackerbarrel.pkl\t\t\n",
    "- hm.pkl\t\t\t\t\n",
    "- store_df.pkl\n",
    "- counties_fixed.geojson\t\t\n",
    "- dicks.csv\t\t\t\n",
    "- hobbylobby.pkl\t\t\t\n",
    "- target.csv\n",
    "- county_2018.csv\t\t\t\n",
    "- district_geojson.pkl\t\t\n",
    "- llbean.csv\t\t\t\n",
    "- tractorsupply.csv\n",
    "- county_boundaries.json\t\t\n",
    "- dollartree.csv\t\t\t\n",
    "- potterybarn.csv\t\t\t\n",
    "- traderjoes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add in a chain stores\n",
    "# COMMENTED OUT CODE TO IMPORT STORES.  TRYING TO DO IT ONE AT A TIME\n",
    "# Each dataset was a little bit different, so I decided not to automate with functions\n",
    "\n",
    "# # WHOLE FOODS \n",
    "# wf_df = pd.read_csv('datasets/wholefoods.csv', encoding='latin')\n",
    "# # wf_df.to_csv('datasets/wholefoods.csv')  # fixing the encoding issue\n",
    "# wf_df['longlat'] = list(zip(wf_df['long'], wf_df['lat']))\n",
    "# wf_df['points'] = wf_df['longlat'].apply(make_point)\n",
    "\n",
    "\n",
    "# # TRACTOR SUPPLY\n",
    "# ts_df = pd.read_csv('datasets/tractorsupply.csv')\n",
    "# ts_df.info()\n",
    "# ts_df = add_longlat(ts_df)\n",
    "# ts_df.describe()\n",
    "\n",
    "\n",
    "# # STARBUCKS\n",
    "# sb_df = pd.read_csv('datasets/starbucks.csv')\n",
    "# sb_df = sb_df.rename(columns={'Longitude': \"long\", \"Latitude\":'lat'})\n",
    "# sb_df = sb_df[sb_df['Country']=='US']\n",
    "# sb_df = add_longlat(sb_df)\n",
    "\n",
    "\n",
    "# # ACADEMY SPORTS\n",
    "# as_df = pd.read_csv('datasets/academy.csv')\n",
    "# as_df = add_longlat(as_df)\n",
    "# as_df.describe()\n",
    "\n",
    "# # DICK'S SPORTS\n",
    "# ds_df = pd.read_csv('datasets/dicks.csv')\n",
    "# ds_df = add_longlat(ds_df)\n",
    "# ds_df.describe()\n",
    "\n",
    "\n",
    "# # BASS PRO SHOPS\n",
    "# bp_df = pd.read_csv('datasets/basspro.csv')\n",
    "# bp_df = add_longlat(bp_df)\n",
    "# bp_df.describe()\n",
    "\n",
    "# # DOLLAR TREE STORE\n",
    "# dt_df = pd.read_csv('datasets/dollartree.csv')\n",
    "# dt_df = add_longlat(dt_df)\n",
    "# dt_df.describe()\n",
    "\n",
    "# # GRAINGER STORE\n",
    "# grg_df = pd.read_csv('datasets/grainger.csv')\n",
    "# grg_df.info()\n",
    "# grg_df = add_longlat(grg_df)  # this one had a bunch of garbage and extra commas\n",
    "# grg_df.describe()\n",
    "\n",
    "\n",
    "# # CHICK FIL A\n",
    "# cf_df = pd.read_csv('datasets/chickfila.csv', encoding='latin')\n",
    "# cf_df.info()\n",
    "# #cf_df.to_csv('datasets/chickfila.csv')\n",
    "# #time.sleep(5)\n",
    "# #cf_df = pd.read_csv('datasets/chickfila.csv', encoding='latin')\n",
    "# cf_df = add_longlat(cf_df)  # format was wrong, had to resave as utf-8\n",
    "# cf_df.describe()\n",
    "\n",
    "\n",
    "# # CRACKER BARREL\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/crackerbarrel.pkl','rb') as f:\n",
    "#     cb_df = pickle.load(f)\n",
    "\n",
    "# cb_df = pd.DataFrame(cb_df)\n",
    "# cb_df.columns = ['long', 'lat']\n",
    "# cb_df = add_longlat(cb_df)\n",
    "# cb_df.info()\n",
    "\n",
    "\n",
    "# # HARLEY DAVIDSON\n",
    "# hd_df = pd.read_csv('datasets/harley.csv', encoding='latin')\n",
    "# #dt_hd.to_csv('datasets/harley.csv')  # did this to fix encoding\n",
    "# hd_df = add_longlat(hd_df)\n",
    "# hd_df.describe()\n",
    "\n",
    "\n",
    "# # H&M\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/hm.pkl','rb') as f:\n",
    "#     hm_df = pickle.load(f)\n",
    "\n",
    "# hm_df = pd.DataFrame(hm_df)\n",
    "# hm_df.columns = ['long', 'lat']\n",
    "# hm_df = add_longlat(hm_df)\n",
    "# hm_df.info()\n",
    "\n",
    "\n",
    "# # Hobby Lobby\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/hobbylobby.pkl','rb') as f:\n",
    "#     hl_df = pickle.load(f)\n",
    "\n",
    "# hl_df = pd.DataFrame(hl_df)\n",
    "# hl_df.columns = ['long', 'lat']\n",
    "# hl_df = add_longlat(hl_df)\n",
    "# hl_df.info()\n",
    "\n",
    "# # LL Bean\n",
    "# ll_df = pd.read_csv('datasets/llbean.csv')\n",
    "# ll_df.info()\n",
    "# ll_df = add_longlat(ll_df)  \n",
    "# ll_df.describe()\n",
    "\n",
    "\n",
    "# # Pottery Barn\n",
    "# pb_df = pd.read_csv('datasets/potterybarn.csv')\n",
    "# pb_df.info()\n",
    "# pb_df = add_longlat(pb_df)  \n",
    "# pb_df.describe()\n",
    "\n",
    "\n",
    "# # REI\n",
    "# rei_df = pd.read_csv('datasets/rei.csv')\n",
    "# rei_df.info()\n",
    "# rei_df = add_longlat(rei_df)  \n",
    "# rei_df.describe()\n",
    "\n",
    "\n",
    "# # Target\n",
    "# tg_df = pd.read_csv('datasets/target.csv', encoding='latin')\n",
    "# tg_df = tg_df.rename(columns={'Address.Longitude': \"long\", \"Address.Latitude\":'lat'})\n",
    "# tg_df.info()\n",
    "# #tg_df.to_csv('datasets/target.csv')  # did this to fix encoding\n",
    "# tg_df = add_longlat(tg_df)  \n",
    "# tg_df.describe()\n",
    "\n",
    "\n",
    "# # Trader Joe's\n",
    "# tj_df = pd.read_csv('datasets/traderjoes.csv')\n",
    "# tj_df.info()\n",
    "# tj_df = add_longlat(tj_df)  \n",
    "# tj_df.describe()\n",
    "\n",
    "# # WalMart. \n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/walmart.pkl','rb') as f:\n",
    "#     wal_df = pickle.load(f)\n",
    "\n",
    "# wal_df = pd.DataFrame(wal_df)\n",
    "\n",
    "# wal_df\n",
    "# wal_df.columns = ['long', 'lat']\n",
    "# wal_df = add_longlat(wal_df)\n",
    "# wal_df.info()\n",
    "\n",
    "\n",
    "# COSTCO\n",
    "cc_df = pd.read_csv('datasets/costco.csv', encoding='latin')\n",
    "#cc_df = cc_df.drop(columns=['Unnamed: 0'])\n",
    "#cc_df.to_csv('datasets/costco.csv')  # did this to fix encoding\n",
    "cc_df.info()\n",
    "cc_df = add_longlat(cc_df)  \n",
    "cc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_county(point, us_county):\n",
    "    for i in range(len(us_county)):\n",
    "        poly = us_county.iloc[i]['geometry']\n",
    "        if poly.contains(point):\n",
    "            return us_county.iloc[i]['fips']\n",
    "        \n",
    "def get_stores_by_county(store_df, us_county):\n",
    "    found = store_df['points'].apply(get_county, args=[us_county])\n",
    "    return found.value_counts()\n",
    "\n",
    "\n",
    "# <<<<<<<<<<<<HIGH RESOURCE CODE ALERT>>>>>>>>>>>>\n",
    "#wf_counts = get_stores_by_county(wf_df, us_county)  # WHOLE FOODS\n",
    "#ts_counts = get_stores_by_county(ts_df, us_county)  # TRACTOR SUPPLY STORE\n",
    "#sb_counts = get_stores_by_county(sb_df, us_county)  # STARBUCKS COFFEE \n",
    "#as_counts = get_stores_by_county(as_df, us_county)  # ACADEMY SPORTS  \n",
    "#ds_counts = get_stores_by_county(ds_df, us_county)  # DICK'S SPORTS\n",
    "#bp_counts = get_stores_by_county(bp_df, us_county)  # BASS PRO\n",
    "#dt_counts = get_stores_by_county(dt_df, us_county)  # DOLLAR TREE\n",
    "#grg_counts = get_stores_by_county(grg_df, us_county)  # GRAINGER\n",
    "#cf_counts = get_stores_by_county(cf_df, us_county)  # CHICK-FIL-A\n",
    "#cb_counts = get_stores_by_county(cb_df, us_county)  # CRACKER BARREL\n",
    "#hd_counts = get_stores_by_county(hd_df, us_county)  # HARLEY DAVIDSON\n",
    "#hm_counts = get_stores_by_county(hm_df, us_county)  # H and M\n",
    "#hl_counts = get_stores_by_county(hl_df, us_county)  # HOBBY LOBBY\n",
    "#ll_counts = get_stores_by_county(ll_df, us_county)  # LL BEAN\n",
    "#pb_counts = get_stores_by_county(pb_df, us_county)   # POTTERY BARN\n",
    "#rei_counts = get_stores_by_county(rei_df, us_county)   # REI\n",
    "#tg_counts = get_stores_by_county(tg_df, us_county)   # TARGET\n",
    "#tj_counts = get_stores_by_county(tj_df, us_county)   # TRADER JOE'S\n",
    "#wal_counts = get_stores_by_county(wal_df, us_county)   # WALMART\n",
    "#cc_counts = get_stores_by_county(cc_df, us_county)   # COSTCO\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the resource intensive code is already run and has been pickled\n",
    "\n",
    "#  Open the pickle jar\n",
    "with open('datasets/store_df.pkl','rb') as f:\n",
    "    df_stores = pickle.load(f)\n",
    "\n",
    "\n",
    "# Implement this as .apply function later\n",
    "\n",
    "# def is_blue(fips, df):\n",
    "#     try:\n",
    "#         blue = df.loc[df['fips']==fips]\n",
    "#         return blue.value\n",
    "#     except Exception as e:\n",
    "#         print(fips, e)\n",
    "\n",
    "# something like this\n",
    "# df_stores['fips'].apply(is_blue, args=(df2_final,)) \n",
    "# df_stores['fips'].apply(is_blue, df2_final)\n",
    "\n",
    "\n",
    "# For now\n",
    "blue = []\n",
    "\n",
    "for i in range(len(df_stores)):\n",
    "    f = df_stores.iloc[i,6]  # hardcoded iloc to get fips code\n",
    "    b = df2_final[df2_final['fips']==str(f)]['blue']\n",
    "    blue.append(int(b))\n",
    "\n",
    "df_stores['blue'] = blue\n",
    "df_stores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def return_count(fips, wf_counts):\n",
    "    try:\n",
    "        return wf_counts[int(fips)]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "# These were added one at a time\n",
    "# df_stores['WholeFoods'] = df_stores.fips.apply(return_count, args=[wf_counts])\n",
    "# df_stores['TractorSupply'] = df_stores.fips.apply(return_count, args=[ts_counts])\n",
    "# df_stores['Starbucks'] = df_stores.fips.apply(return_count, args=[sb_counts])\n",
    "# df_stores['AcademySports'] = df_stores.fips.apply(return_count, args=[as_counts])\n",
    "# df_stores['DicksSports'] = df_stores.fips.apply(return_count, args=[ds_counts])\n",
    "# df_stores['BassPro'] = df_stores.fips.apply(return_count, args=[bp_counts])\n",
    "# df_stores['DollarTree'] = df_stores.fips.apply(return_count, args=[dt_counts])\n",
    "# df_stores['Grainger'] = df_stores.fips.apply(return_count, args=[grg_counts])\n",
    "# df_stores['ChickFila'] = df_stores.fips.apply(return_count, args=[cf_counts])\n",
    "# df_stores['CrackerBarrel'] = df_stores.fips.apply(return_count, args=[cb_counts])\n",
    "# df_stores['HarleyDavidson'] = df_stores.fips.apply(return_count, args=[hd_counts])\n",
    "# df_stores['HM'] = df_stores.fips.apply(return_count, args=[hm_counts])\n",
    "# df_stores['HobbyLobby'] = df_stores.fips.apply(return_count, args=[hl_counts])\n",
    "#df_stores['LLBean'] = df_stores.fips.apply(return_count, args=[ll_counts])\n",
    "#df_stores['PotteryBarn'] = df_stores.fips.apply(return_count, args=[pb_counts])\n",
    "#df_stores['REI'] = df_stores.fips.apply(return_count, args=[rei_counts])\n",
    "#df_stores['Target'] = df_stores.fips.apply(return_count, args=[tg_counts])\n",
    "#df_stores['TraderJoes'] = df_stores.fips.apply(return_count, args=[tj_counts])\n",
    "#df_stores['Walmart'] = df_stores.fips.apply(return_count, args=[wal_counts])\n",
    "#df_stores['Costco'] = df_stores.fips.apply(return_count, args=[cc_counts])\n",
    "\n",
    "\n",
    "# UNCOMMENT THIS LINE TO SAVE THE DATA AGAIN\n",
    "#df_stores.to_pickle(\"datasets/store_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quick imported geojson file for plotting\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    us_county_fix = json.load(response)\n",
    "\n",
    "# EACH LINE MAKES A STORE CHOROPLETH MAP FOR US\n",
    "# Each also is 'tuned' to best display store density\n",
    "\n",
    "#make_choro(df_stores1, us_county_fix, 'WholeFoods', ['lightgray', 'green'], [0, 4])\n",
    "#make_choro(df_stores2, us_county_fix, 'TractorSupply', ['lightgray', 'red'], [0, 4])\n",
    "#make_choro(df_stores5, us_county_fix, 'Starbucks', ['white', 'green'], [0, 50])\n",
    "#make_choro(df_stores5, us_county_fix, 'AcademySports', ['white', 'blue'], [0, 2])\n",
    "#make_choro(df_stores5, us_county_fix, 'DicksSports', ['white', 'orange'], [0, 2])\n",
    "#make_choro(df_stores7, us_county_fix, 'BassPro', ['lightyellow', 'red'], [0, 2])\n",
    "#make_choro(df_stores7, us_county_fix, 'DollarTree', ['white', 'green'], [0, 50])\n",
    "#make_choro(df_stores, us_county_fix, 'Grainger', ['lightgray', 'red'], [0, 3])\n",
    "#make_choro(df_stores, us_county_fix, 'ChickFila', ['lightgray', 'red'], [0, 20])\n",
    "#make_choro(df_stores, us_county_fix, 'CrackerBarrel', ['lightgray', 'gold'], [0, 4])\n",
    "#make_choro(df_stores, us_county_fix, 'HarleyDavidson', ['lightgray', 'orange'], [0, 2])\n",
    "#make_choro(df_stores, us_county_fix, 'HM', ['white', 'red'], [0, 2])\n",
    "#make_choro(df_stores, us_county_fix, 'HobbyLobby', ['white', 'orange'], [0, 3])\n",
    "#make_choro(df_stores, us_county_fix, 'LLBean', ['white', 'darkgreen'], [0, 1])\n",
    "#make_choro(df_stores, us_county_fix, 'PotteryBarn', ['white', 'blue'], [0, 1])\n",
    "#make_choro(df_stores, us_county_fix, 'REI', ['white', 'darkgreen'], [0, 1])\n",
    "#make_choro(df_stores, us_county_fix, 'Target', ['white', 'red'], [0, 10])\n",
    "#make_choro(df_stores, us_county_fix, 'TraderJoes', ['white', 'red'], [0, 5])\n",
    "#make_choro(df_stores, us_county_fix, 'Costco', ['white', 'blue'], [0, 1])\n",
    "\n",
    "#make_choro(df2_final, us_county_fix, 'blue', ['red', 'blue'], [0, 1])\n",
    "df_stores.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete my df_store and make it final, I want to do a couple experiments\n",
    "# I want to simply add a population col\n",
    "# I would like to make a second model using stores/person for each column\n",
    "\n",
    "# Add population data\n",
    "pop_df = pd.read_csv('datasets/county_pop.csv', encoding='latin')\n",
    "#pop_df.to_csv('datasets/county_pop.csv')\n",
    "\n",
    "# pop_df.info()\n",
    "# for col in pop_df.columns:\n",
    "#     print(col)\n",
    "    \n",
    "# Keep STATE, COUNTY, CTYNAME, POPESTIMATE2018, \n",
    "pop_df = pop_df[['STATE', 'COUNTY', 'CTYNAME', 'POPESTIMATE2018']]\n",
    "pop_df.head()\n",
    "fips_list = []\n",
    "\n",
    "for i in range(len(pop_df)):\n",
    "    fips = \"{:02}{:03}\".format(pop_df.iloc[i]['STATE'], pop_df.iloc[i]['COUNTY'])\n",
    "    fips_list.append(fips)\n",
    "    \n",
    "pop_df['fips'] = pd.Series(fips_list)\n",
    "\n",
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add population and area columns to my final dataset\n",
    "# Helper functions in the import\n",
    "def get_area(fips, us_county, area_index):\n",
    "    try:\n",
    "        area = us_county[us_county['fips']==fips].iloc[:, area_index]\n",
    "        return float(area)\n",
    "    except Exception as e:\n",
    "        print(fips, e)\n",
    "\n",
    "        \n",
    "\n",
    "us_county['fips'] = us_county['fips'].apply(lambda x: \"{:05}\".format(int(x)))\n",
    "area_index = list(us_county.columns).index('CENSUSAREA')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_stores['population'] = df_stores.fips.apply(get_pop, args=[pop_df])\n",
    "df_stores['area'] = df_stores.fips.apply(get_area, args=[us_county, area_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE EXCEPTION THROWN....\n",
    "# the population of bedford city county va (fips 51515) is 6181.  Let's fill it in manually\n",
    "df_stores.loc[df_stores['fips']=='51515']\n",
    "df_stores.loc[5422,'population'] = 6181\n",
    "df_stores.loc[df_stores['fips']=='51515']  # fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good, let's pickle it and start our ML project\n",
    "\n",
    "# UNCOMMENT THE FILE YOU WANT TO PICKLE TO\n",
    "# may want to automate this later with a variable depending on offices chosen\n",
    "\n",
    "#df_stores.to_pickle(\"datasets/house_df.pkl\")\n",
    "df_stores.to_pickle('datasets/all_df.pkl')\n",
    "#df_stores.to_pickle('datasets/major_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little extra to fix the df before finally sending it to pickle\n",
    "# This code was run once only to fix the dataset after loading the store data\n",
    "\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# # import data\n",
    "\n",
    "# with open('datasets/final_df.pkl', 'rb') as f:\n",
    "#     df = pickle.load(f)\n",
    "    \n",
    "# #had an extra column and used this to fix it\n",
    "# df = df.drop(df.columns[-1], axis=1)\n",
    "\n",
    "\n",
    "# df.party = df.party.apply(lambda x: x.title())\n",
    "# df.county = df.county.apply(lambda x: x.title())\n",
    "# # df = df.drop(columns=['totalvotes'], axis=1)\n",
    "\n",
    "# df.head()\n",
    "# #df.to_pickle('datasets/final_df.pkl')\n",
    "# df.to_pickle('datasets/house_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
