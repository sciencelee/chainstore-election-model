{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start by getting the county information\n",
    "\n",
    "All of my data is linked by the FIPS identifying code for each county.  \n",
    "We will be using the FIPS data for plotting and id of county bounds and as a defacto index across all datasets.\n",
    "\n",
    "We will also be using the county name for additional identification and the census tract area for each county.\n",
    "The census area is the surveyed area for census purposes (where people live) within the bounds of the county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_county' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dc5e1cdf4133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mus_county\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_county\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/county_boundaries.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_county' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the county fips codes\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "\n",
    "us_county_path = 'datasets/county_boundaries.json'\n",
    "\n",
    "cur_json = json.load(open(us_county_path, encoding='ISO-8859-1'))\n",
    "path,ext = os.path.splitext(us_county_path)\n",
    "new_path = path+\"_new\"+ext\n",
    "\n",
    "with open(new_path,\"w\", encoding='utf-8') as jsonfile:\n",
    "    json.dump(cur_json,jsonfile,ensure_ascii=False)\n",
    "\n",
    "us_county = gpd.read_file(new_path, driver='GeoJSON')\n",
    "\n",
    "us_county['fips'] = us_county['STATE'] + us_county['COUNTY']\n",
    "us_county = us_county[us_county['STATE'].apply(int) < 57]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>NAME</th>\n",
       "      <th>CENSUSAREA</th>\n",
       "      <th>fips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>594.436</td>\n",
       "      <td>01001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>009</td>\n",
       "      <td>Blount</td>\n",
       "      <td>644.776</td>\n",
       "      <td>01009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>017</td>\n",
       "      <td>Chambers</td>\n",
       "      <td>596.531</td>\n",
       "      <td>01017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>021</td>\n",
       "      <td>Chilton</td>\n",
       "      <td>692.854</td>\n",
       "      <td>01021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01</td>\n",
       "      <td>033</td>\n",
       "      <td>Colbert</td>\n",
       "      <td>592.619</td>\n",
       "      <td>01033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  STATE COUNTY      NAME  CENSUSAREA   fips\n",
       "0    01    001   Autauga     594.436  01001\n",
       "1    01    009    Blount     644.776  01009\n",
       "2    01    017  Chambers     596.531  01017\n",
       "3    01    021   Chilton     692.854  01021\n",
       "4    01    033   Colbert     592.619  01033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_county_df = pd.DataFrame(us_county[['STATE', 'COUNTY', 'NAME', 'CENSUSAREA','fips']])\n",
    "us_county_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 Midterm Election Data\n",
    "\n",
    "The election data is taken from https://electionlab.mit.edu/data.\n",
    "The data has results for every election, both state and national.  It includes the vote total for each candidate and the candidate's party.\n",
    "\n",
    "We will use the fips code for each county to act as an index for the more than 3000 counties contained in the dataset.  Fips codes have a two digit state id followed by a three digit county id code within the state.  States are listed in alphabetical order.  The dataset also contains information on territories (Guam, NMI, PR, etc.) but that will be filtered out to focus on the contiguous United States and Hawaii.  The burroughs of Alaska are unfortunately not part of this dataset, although after looking at the frequency of stores in Alaska, it might  not be useful to add to the model as many of these stores are not present or sparse in our 49th state.  District of Columbia is unfortunately not included.  Adding the Alaskan burroughs and DC would be a recommendation for further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  5,  6,  8,  9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "       23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in election results from csv\n",
    "df = pd.read_csv('datasets/county_2018.csv', encoding='latin') \n",
    "# There was an encoding error that prevented the dataset from importing properly.  \n",
    "# It was imported, written back as a csv and imported again without problems.\n",
    "\n",
    "# Eliminates territories (past Wyoming last alphabetically)\n",
    "df = df[df['state_fips'].apply(int) < 57]\n",
    "df['state_fips'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow the election data \n",
    "I chose to go with only the US House races.  \n",
    "Senators tend to have long term support statewide (in some states) and might influence the model.\n",
    "\n",
    "Further investigation: you could look at all races cumulative to determine red or blue counties.\n",
    "Many local elections are decided cross party, unlike congressional elections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 110492 entries, 6 to 177663\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   state           110492 non-null  object \n",
      " 1   county          110482 non-null  object \n",
      " 2   state_fips      110492 non-null  int64  \n",
      " 3   party           110492 non-null  object \n",
      " 4   candidatevotes  110058 non-null  float64\n",
      " 5   totalvotes      108089 non-null  float64\n",
      " 6   office          110492 non-null  object \n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# remove all lines that are not republican or democrat\n",
    "df.party.unique() # two party data only, there are lots of them \n",
    "df2 = df[(df['party']=='republican') | (df['party']=='democrat')]  # df2 is two party only\n",
    "df2.party.value_counts()\n",
    "\n",
    "\n",
    "# I would like to see if model perfoms better on national elections.  \n",
    "# Might show partisan lean better than state and local\n",
    "all_offices = ['Associate Justice of the Supreme Court, Place 1',\n",
    "       'Associate Justice of the Supreme Court, Place 2',\n",
    "       'Associate Justice of the Supreme Court, Place 3',\n",
    "       'Associate Justice of the Supreme Court, Place 4',\n",
    "       'Attorney General', 'Chief Justice of the Supreme Court',\n",
    "       'Commissioner of Agriculture and Industries', 'Governor',\n",
    "       'Lieutenant Governor', 'Public Service Commission, Place 1',\n",
    "       'Public Service Commission, Place 2', 'Secretary of State',\n",
    "       'State Auditor', 'State Representative', 'State Senator',\n",
    "       'State Treasurer', 'US Representative',\n",
    "       'State Board of Education Member', 'Corporation Commissioner',\n",
    "       'State Mine Inspector', 'Superintendant of Public Instruction',\n",
    "       'US Senator', 'Auditor of State', 'Commissioner of State Lands',\n",
    "       'State Senate', 'Board of Equalization Member', 'Controller',\n",
    "       'Insurance Commissioner', 'State Assembly Member', 'Treasurer',\n",
    "       'Governor/Lieutenant Governor',\n",
    "       'Regent of the University of Colorado', 'Comptroller',\n",
    "       'Governor and Lieutenant Governor', 'Secretary of the State',\n",
    "       'Auditor of Accounts', 'Chief Financial officer',\n",
    "       'Commissioner of Agriculture', 'State Attorney',\n",
    "       'Commissioner of Insurance', 'Commissioner of Labor',\n",
    "       'Public Service Commission, District 3 - Metro-Atlanta',\n",
    "       'Public Service Commission, District 5 - Western',\n",
    "       'State School Superintendent', 'State Controller',\n",
    "       'State Representative A', 'State Representative B',\n",
    "       'Superintendent of Public Instruction', 'Treasurer of State',\n",
    "       'Secretary of Agriculture', 'Governor / Lt. Governor',\n",
    "       'House of Delegates Member', 'Auditor', \"Governor's Council\",\n",
    "       'Secretary of the Commonwealth',\n",
    "       'Member of the State Board of Education',\n",
    "       'Regent of the University of Michigan',\n",
    "       'State Representative (Partial Term Ending 01/01/2019)',\n",
    "       'State Senator (Partial Term Ending 01/01/2019)',\n",
    "       'US Representative (Partial Term Ending 01/03/2019)',\n",
    "       'Governor & Lt Governor', 'Auditor of Public Accounts',\n",
    "       'Governor and Lt. Governor', 'Public Service Commissioner',\n",
    "       'Executive Council', 'Commissioner of Public Lands',\n",
    "       'Justice of the Supreme Court', 'Supreme Court Justice',\n",
    "       'NC Supreme Court Associate Justice Seat 1',\n",
    "       'Agriculture Commissioner', 'For Attorney General',\n",
    "       'For Corporation Commissioner', 'For Insurance Commissioner',\n",
    "       'State Auditor and Inspector', 'General Treasurer',\n",
    "       'Comptroller General', 'State Superintendent of Education',\n",
    "       'Commissioner School Public Lands',\n",
    "       'Public Utilities Commissioner',\n",
    "       'Commissioner of the General Land office',\n",
    "       'Comptroller of Public Accounts',\n",
    "       'Judge, Court of Criminal Appeals Place 7',\n",
    "       'Judge, Court of Criminal Appeals Place 8',\n",
    "       'Justice, Supreme Court, Place 2',\n",
    "       'Justice, Supreme Court, Place 4',\n",
    "       'Justice, Supreme Court, Place 6',\n",
    "       'Presiding Judge, Court of Criminal Appeals',\n",
    "       'Railroad Commissioner', 'Member, State Board of Education',\n",
    "       'State Representative Pos. 1', 'State Representative Pos. 2',\n",
    "       'State House Delegate', 'State Assembly Representative']\n",
    "\n",
    "major_offices = ['Governor',\n",
    "                 'US Representative',\n",
    "                 'US Senator',\n",
    "                 'Governor/Lieutenant Governor',\n",
    "                 'Governor and Lieutenant Governor',\n",
    "                 'Governor / Lt. Governor',\n",
    "                 'US Representative (Partial Term Ending 01/03/2019)',\n",
    "                 'Governor & Lt Governor', \n",
    "                 'Governor and Lt. Governor', \n",
    "                 'For Attorney General'\n",
    "                ]\n",
    "\n",
    "congress =      [\n",
    "                 'US Representative',\n",
    "                 'US Senator',\n",
    "                 'US Representative (Partial Term Ending 01/03/2019)',\n",
    "                ]\n",
    "\n",
    "house = [\n",
    "                 'US Representative',\n",
    "                 'US Representative (Partial Term Ending 01/03/2019)',\n",
    "        ]\n",
    "\n",
    "# # Put in the data you wish to include in the model.  You could use congress, major, or all (I chose house)\n",
    "df_temp = df2.copy()\n",
    "#df2 = df_temp[(df_temp['office'].isin(all_offices)]\n",
    "df2 = df2[df2['office'].isin(all_offices)]\n",
    "\n",
    "\n",
    "\n",
    "# eliminate unnecessary columns\n",
    "df2 = df2[['state', 'county', 'state_fips', 'party', 'candidatevotes', 'totalvotes', 'office']]\n",
    "\n",
    "df2.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'republican', 'democrat', 'libertarian', 'independent',\n",
       "       'green', 'nonpartisan', 'no party preference', 'unity',\n",
       "       'american constitution', 'approval voting', 'unaffiliated',\n",
       "       'working families', 'amigo constitution liberty',\n",
       "       'griebel frank for ct', 'petitioning candidate',\n",
       "       'no party affiliation', 'reform party of florida',\n",
       "       'libertarian party of florida', 'green party', 'constitution',\n",
       "       'conservative', 'downstate united', 'clear water',\n",
       "       'legal medical now', 'no party', 'green independent',\n",
       "       'candid common sense', 'common sense independent',\n",
       "       \"people's unenrolled independent\", 'independent for maine',\n",
       "       'unenrolled', 'maine socialist party', 'green-rainbow',\n",
       "       'massachusetts independent', 'second american revolution',\n",
       "       'cooperative green economy', 'independent and veteran',\n",
       "       'independent progressive', 'no  affiliation', 'us taxpayers',\n",
       "       'natural law', 'working class', 'democratic-farmer-labor',\n",
       "       'grassroots-legalize cannabis', 'independence',\n",
       "       'legal marijuana now', 'minnesota green', 'reform',\n",
       "       'independent american', 'democrat&republican',\n",
       "       'democrat/republican', 'cannot be bought', 'time for truth',\n",
       "       'hope in unity', 'economic growth', 'make it simple',\n",
       "       'for the people', 'new day nj', 'trade, health, environment',\n",
       "       'new way forward', 'your voice hard', 'we deserve better',\n",
       "       'together we can', 'stop the insanity', 'c4c 2018',\n",
       "       'never give up', 'honesty, integrity, compassion',\n",
       "       'freedom, responsibility, action',\n",
       "       'integrity transparency accountability', 'repeal bail reform',\n",
       "       'time for change', 'check this column', 'ed the barber',\n",
       "       'the inclusion candidate', 'bringing back manufacturing',\n",
       "       \"women's equality\", 'tax revolt', 'in maio we trust',\n",
       "       'democratic-npl', 'independent nomination', 'pacific green',\n",
       "       'democratic / republican', 'no affiliation', 'compassion',\n",
       "       'moderate', 'a better rhode island', 'united citizens', 'american',\n",
       "       'united utah', 'liberty union', 'dem/prog', 'earth rights',\n",
       "       'prog/dem', 'rep/dem', 'dem/rep', 'progressive',\n",
       "       'fair representation vt', 'berlin-northfield alliance',\n",
       "       'green mountain', 'independent republican', 'mountain'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.party.unique() # two party data only, there are lots of them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6098 entries, 0 to 6097\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   state_fips      6098 non-null   int64  \n",
      " 1   county          6098 non-null   object \n",
      " 2   party           6098 non-null   object \n",
      " 3   candidatevotes  6098 non-null   float64\n",
      " 4   totalvotes      6098 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 285.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# We decided to just count up all of the votes for each party in every election.\n",
    "# If we chose to include local elections or major elections, this would\n",
    "# weight congressional elections the same as local, but is aimed at getting a sense of how much\n",
    "# the county leans red or blue without individual politicians affecting the categorization\n",
    "df2_grouped = df2.groupby(by=['state_fips', 'county', 'party']).sum().reset_index()\n",
    "\n",
    "# This drops out territories by chopping off everything after Wyoming\n",
    "df2_grouped = df2_grouped[df2_grouped['state_fips'].apply(int) < 57]\n",
    "df2_grouped.info()\n",
    "\n",
    "# we are left with vote totals for GOP and Dem for each county\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the county data\n",
    "County naming is not standardized.  There were lots of exceptions in the more than 3000 counties that had to be addressed.  The data in the us_county dataset was not named the same as election data.  Each .replace was a manual change to get the two datasets to conform.  The election data did not have FIPS data initially included.  That is unfortunate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new col named fips which is initially filled with zeroes\n",
    "df2_grouped_fips = df2_grouped.copy()\n",
    "df2_grouped_fips['fips'] = pd.Series([0 for x in range(len(df2_grouped_fips.index))])  \n",
    "\n",
    "\n",
    "# The code below ensures that county names match exactly so we can populate the fips column\n",
    "# this will make fips a key used for election, county, and population datasets\n",
    "us_county_df['NAME'] = us_county_df['NAME'].apply(lambda x: x.upper())\n",
    "us_county_df['NAME'] = us_county_df['NAME'].apply(lambda x: x.replace('.', '')\n",
    "                                                              .replace(\"''\", '')\n",
    "                                                              .replace(\"DE WITT\", 'DEWITT')\n",
    "                                                              .replace('LA SALLE', 'LASALLE')\n",
    "                                                              .replace(\"DE KALB\", 'DEKALB')\n",
    "                                                              .replace(\" CITY\", '')\n",
    "                                                              .strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2_grouped_fips['county'] = df2_grouped_fips['county'].apply(lambda x: x.upper())\n",
    "df2_grouped_fips['county'] = df2_grouped_fips['county'].apply(lambda x: x.replace('COUNTY', '')\n",
    "                                                                          .replace('.', '')\n",
    "                                                                          .replace(' CITY', '')\n",
    "                                                                          .replace('MEEER', 'MEEKER')\n",
    "                                                                          .replace('JODAVIESS', 'JO DAVIESS')\n",
    "                                                                          .replace('&', 'AND')\n",
    "                                                                          .replace('DE WITT', 'DEWITT')\n",
    "                                                                          .replace('DE KALB', 'DEKALB')\n",
    "                                                                          .replace('LAC QUI PARTE', 'LAC QUI PARLE')\n",
    "                                                                          .replace('OGLALA LAKOTA', 'OGLALA')\n",
    "                                                                          .replace('CHENAGO', 'CHENANGO')\n",
    "                                                                          .replace('LA SALLE', 'LASALLE')\n",
    "                                                                          .replace('DONA ANA', 'DOÃ‘A ANA')\n",
    "                                                                          .strip())\n",
    "\n",
    "# These are bad county names we dropped\n",
    "# They inclued UOCAVA (overseas votes), Oglala Lakota and some other NON-COUNTY data\n",
    "df2_grouped_fips = df2_grouped_fips.loc[~df2_grouped_fips['county'].isin(['STATE TOTALS', \n",
    "                                                                            'STATE UOCAVA', \n",
    "                                                                            'TOTAL VOTES BY CANDIDATE',\n",
    "                                                                            'TOTAL VOTES BY PARTY',\n",
    "                                                                            'FEDERAL PRECINCT',\n",
    "                                                                            'KANSAS',\n",
    "                                                                            'OGLALA',\n",
    "                                                                        ])]\n",
    "\n",
    "# Force numperic values \n",
    "us_county_df.info()\n",
    "df2_grouped_fips.info()\n",
    "us_county_df[\"STATE\"] = pd.to_numeric(us_county_df[\"STATE\"])\n",
    "us_county_df[\"fips\"] = pd.to_numeric(us_county_df[\"fips\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this loop to go back and correct the county names between the two datasets\n",
    "errors = 0  # track the dumped counties\n",
    "\n",
    "for i in range(len(df2_grouped_fips)):\n",
    "    state = int(df2_grouped_fips.iloc[i, :]['state_fips'])  # get state number (1 to 57 numeric)\n",
    "    county = df2_grouped_fips.iloc[i, :]['county'].strip() # get county name\n",
    "    #print(us_county_df.loc[us_county_df['NAME'] == county].iloc[-1][-1])\n",
    "    try:\n",
    "        # try to associate add the approprate fips for each county\n",
    "        fip = us_county_df.loc[(us_county_df['NAME']==county) & (us_county_df['STATE']==state)].iloc[-1][-1]\n",
    "        df2_grouped_fips.iloc[i, -1] = fip  # BE CAREFUL HERE\n",
    "    except:\n",
    "        # if it didn't work, print it out for troubleshooting\n",
    "        print(county, state, i)\n",
    "        errors +=1\n",
    "        print()\n",
    "        \n",
    "\n",
    "print(errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df2_grouped_fips.copy().reset_index()  # failed at index 414, index was missing for agg(idxmax) (Fixed)\n",
    "\n",
    "\n",
    "# get only highest (DEM or GOP)\n",
    "# This eliminates the 'loser' of each county (this is aggregate votes, not individual elections)\n",
    "df_final = df_final.iloc[df_final.groupby('fips')['candidatevotes'].agg(pd.Series.idxmax)]\n",
    "#df2_final = df2_final.iloc[df2_final.groupby('fips')['candidatevotes'].idxmax().values.ravel()]\n",
    "\n",
    "df_final.describe()\n",
    "\n",
    "\n",
    "# FINAL TALLY\n",
    "# US HAS 3141 total counties.\n",
    "\n",
    "# Missing counties\n",
    "# DC has no counties.  Not sure how to handle that\n",
    "# MISSING OGLALA LAKOTA county (Native American lands)\n",
    "# MISSING the 19 ALASKA buroughs data \n",
    "# MAY BE MISSING MORE COUNTY EQUIVALENTS\n",
    "\n",
    "# Data is missing for Iowa US Congressional race, must use all offices instead.  bummer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a county election map\n",
    "County boundaries found at https://eric.clst.org/tech/usgeojson/\n",
    "There was an encoding error which was fixed using instructions from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import json\n",
    "# import os\n",
    "# import geopandas as gpd\n",
    "\n",
    "# us_county_path = 'datasets/county_boundaries.json'\n",
    "\n",
    "# cur_json = json.load(open(us_county_path, encoding='ISO-8859-1'))\n",
    "# path,ext = os.path.splitext(us_county_path)\n",
    "\n",
    "# new_path =path+\"_new\"+ext\n",
    "# with open(new_path,\"w\", encoding='utf-8') as jsonfile:\n",
    "#     json.dump(cur_json,jsonfile,ensure_ascii=False)\n",
    "\n",
    "# us_county = gpd.read_file(new_path, driver='GeoJSON')\n",
    "\n",
    "# us_county['fips'] = us_county['STATE'] + us_county['COUNTY']\n",
    "\n",
    "\n",
    "# type(us_county)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_final = df_final.copy()\n",
    "\n",
    "\n",
    "\n",
    "# MAKE MY FIPS COMPATIBLE WITH THE GEODATA\n",
    "df2_final['fips'] = df2_final['fips'].apply(lambda x: \"{:05}\".format(x))\n",
    "\n",
    "df2_final['blue'] = df2_final['party'].apply(lambda x: 0 if (x=='republican') else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_final.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# center = [37.0373, -95.6164]\n",
    "# bins = list(df2_final['candidatevotes'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1]))  # <<<<<<< ADD THIS LINE\n",
    "\n",
    "\n",
    "# # Initialize Folium Map again (same as before)\n",
    "# m = folium.Map(location=center, \n",
    "#                zoom_start=5,\n",
    "#                tiles='Stamen Toner')\n",
    "\n",
    "\n",
    "# # Create choropleth map  \n",
    "# folium.Choropleth(\n",
    "#     geo_data=us_county,\n",
    "#     name='choropleth',\n",
    "#     data=df2_final,\n",
    "#     key_on='feature.properties.fips',\n",
    "#     columns=['fips', 'blue'],\n",
    "#     fill_color='Spectral',\n",
    "#     fill_opacity=0.5,\n",
    "#     nan_fill_opacity=0.5,\n",
    "#     line_opacity=1,\n",
    "#     legend_name='2018 Midterm Election',\n",
    "#     us\n",
    "    \n",
    "# ).add_to(m)\n",
    "\n",
    "\n",
    "# m.save('county_choropleth.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_county.head()\n",
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#us_county.to_file(\"datasets/counties_fixed.geojson\", driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import plotly.express as px\n",
    "# from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# # with open(\"datasets/counties_fixed.geojson\") as f:\n",
    "# #     us_county_fix = json.load(f)\n",
    "\n",
    "# with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "#     us_county_fix = json.load(response)\n",
    "    \n",
    "# fig = px.choropleth(df2_final, \n",
    "#                     geojson=us_county_fix, \n",
    "#                     locations='fips', \n",
    "#                     color='party',\n",
    "#                     color_discrete_sequence=px.colors.qualitative.Set1,\n",
    "#                     #color_continuous_scale=[\"red\", 'blue'],\n",
    "#                     #range_color=(0, 1),\n",
    "#                     scope=\"usa\",\n",
    "#                     hover_name='county'\n",
    "#                     )\n",
    "#fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "#fig.show()\n",
    "          \n",
    "# don't have fips in yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_final.describe()\n",
    "#df2_final.head()\n",
    "#df2_final.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets are stored in the /datasets folder which contains:\n",
    "- academy.csv\t\t\t\n",
    "- county_boundaries_new.json\t\n",
    "- grainger.csv\t\t\t\n",
    "- rei.csv\t\t\t\t\n",
    "- walmart.pkl\n",
    "- basspro.csv\t\t\t\n",
    "- county_fips_master.csv\t\t\n",
    "- harley.csv\t\t\t\n",
    "- starbucks.csv\t\t\t\n",
    "- wholefoods.csv\n",
    "- chickfila.csv\t\t\t\n",
    "- crackerbarrel.pkl\t\t\n",
    "- hm.pkl\t\t\t\t\n",
    "- store_df.pkl\n",
    "- counties_fixed.geojson\t\t\n",
    "- dicks.csv\t\t\t\n",
    "- hobbylobby.pkl\t\t\t\n",
    "- target.csv\n",
    "- county_2018.csv\t\t\t\n",
    "- district_geojson.pkl\t\t\n",
    "- llbean.csv\t\t\t\n",
    "- tractorsupply.csv\n",
    "- county_boundaries.json\t\t\n",
    "- dollartree.csv\t\t\t\n",
    "- potterybarn.csv\t\t\t\n",
    "- traderjoes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Add in a chain stores\n",
    "# COMMENTED OUT CODE TO IMPORT STORES.  TRYING TO DO IT ONE AT A TIME\n",
    "# Each dataset was a little bit different, so I decided not to automate with functions\n",
    "\n",
    "# # WHOLE FOODS \n",
    "# wf_df = pd.read_csv('datasets/wholefoods.csv', encoding='latin')\n",
    "# # wf_df.to_csv('datasets/wholefoods.csv')  # fixing the encoding issue\n",
    "# wf_df['longlat'] = list(zip(wf_df['long'], wf_df['lat']))\n",
    "# wf_df['points'] = wf_df['longlat'].apply(make_point)\n",
    "\n",
    "\n",
    "# # TRACTOR SUPPLY\n",
    "# ts_df = pd.read_csv('datasets/tractorsupply.csv')\n",
    "# ts_df.info()\n",
    "# ts_df = add_longlat(ts_df)\n",
    "# ts_df.describe()\n",
    "\n",
    "\n",
    "# # STARBUCKS\n",
    "# sb_df = pd.read_csv('datasets/starbucks.csv')\n",
    "# sb_df = sb_df.rename(columns={'Longitude': \"long\", \"Latitude\":'lat'})\n",
    "# sb_df = sb_df[sb_df['Country']=='US']\n",
    "# sb_df = add_longlat(sb_df)\n",
    "\n",
    "\n",
    "# # ACADEMY SPORTS\n",
    "# as_df = pd.read_csv('datasets/academy.csv')\n",
    "# as_df = add_longlat(as_df)\n",
    "# as_df.describe()\n",
    "\n",
    "# # DICK'S SPORTS\n",
    "# ds_df = pd.read_csv('datasets/dicks.csv')\n",
    "# ds_df = add_longlat(ds_df)\n",
    "# ds_df.describe()\n",
    "\n",
    "\n",
    "# # BASS PRO SHOPS\n",
    "# bp_df = pd.read_csv('datasets/basspro.csv')\n",
    "# bp_df = add_longlat(bp_df)\n",
    "# bp_df.describe()\n",
    "\n",
    "# # DOLLAR TREE STORE\n",
    "# dt_df = pd.read_csv('datasets/dollartree.csv')\n",
    "# dt_df = add_longlat(dt_df)\n",
    "# dt_df.describe()\n",
    "\n",
    "# # GRAINGER STORE\n",
    "# grg_df = pd.read_csv('datasets/grainger.csv')\n",
    "# grg_df.info()\n",
    "# grg_df = add_longlat(grg_df)  # this one had a bunch of garbage and extra commas\n",
    "# grg_df.describe()\n",
    "\n",
    "\n",
    "# # CHICK FIL A\n",
    "# cf_df = pd.read_csv('datasets/chickfila.csv', encoding='latin')\n",
    "# cf_df.info()\n",
    "# #cf_df.to_csv('datasets/chickfila.csv')\n",
    "# #time.sleep(5)\n",
    "# #cf_df = pd.read_csv('datasets/chickfila.csv', encoding='latin')\n",
    "# cf_df = add_longlat(cf_df)  # format was wrong, had to resave as utf-8\n",
    "# cf_df.describe()\n",
    "\n",
    "\n",
    "# # CRACKER BARREL\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/crackerbarrel.pkl','rb') as f:\n",
    "#     cb_df = pickle.load(f)\n",
    "\n",
    "# cb_df = pd.DataFrame(cb_df)\n",
    "# cb_df.columns = ['long', 'lat']\n",
    "# cb_df = add_longlat(cb_df)\n",
    "# cb_df.info()\n",
    "\n",
    "\n",
    "# # HARLEY DAVIDSON\n",
    "# hd_df = pd.read_csv('datasets/harley.csv', encoding='latin')\n",
    "# #dt_hd.to_csv('datasets/harley.csv')  # did this to fix encoding\n",
    "# hd_df = add_longlat(hd_df)\n",
    "# hd_df.describe()\n",
    "\n",
    "\n",
    "# # H&M\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/hm.pkl','rb') as f:\n",
    "#     hm_df = pickle.load(f)\n",
    "\n",
    "# hm_df = pd.DataFrame(hm_df)\n",
    "# hm_df.columns = ['long', 'lat']\n",
    "# hm_df = add_longlat(hm_df)\n",
    "# hm_df.info()\n",
    "\n",
    "\n",
    "# # Hobby Lobby\n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/hobbylobby.pkl','rb') as f:\n",
    "#     hl_df = pickle.load(f)\n",
    "\n",
    "# hl_df = pd.DataFrame(hl_df)\n",
    "# hl_df.columns = ['long', 'lat']\n",
    "# hl_df = add_longlat(hl_df)\n",
    "# hl_df.info()\n",
    "\n",
    "# # LL Bean\n",
    "# ll_df = pd.read_csv('datasets/llbean.csv')\n",
    "# ll_df.info()\n",
    "# ll_df = add_longlat(ll_df)  \n",
    "# ll_df.describe()\n",
    "\n",
    "\n",
    "# # Pottery Barn\n",
    "# pb_df = pd.read_csv('datasets/potterybarn.csv')\n",
    "# pb_df.info()\n",
    "# pb_df = add_longlat(pb_df)  \n",
    "# pb_df.describe()\n",
    "\n",
    "\n",
    "# # REI\n",
    "# rei_df = pd.read_csv('datasets/rei.csv')\n",
    "# rei_df.info()\n",
    "# rei_df = add_longlat(rei_df)  \n",
    "# rei_df.describe()\n",
    "\n",
    "\n",
    "# # Target\n",
    "# tg_df = pd.read_csv('datasets/target.csv', encoding='latin')\n",
    "# tg_df = tg_df.rename(columns={'Address.Longitude': \"long\", \"Address.Latitude\":'lat'})\n",
    "# tg_df.info()\n",
    "# #tg_df.to_csv('datasets/target.csv')  # did this to fix encoding\n",
    "# tg_df = add_longlat(tg_df)  \n",
    "# tg_df.describe()\n",
    "\n",
    "\n",
    "# # Trader Joe's\n",
    "# tj_df = pd.read_csv('datasets/traderjoes.csv')\n",
    "# tj_df.info()\n",
    "# tj_df = add_longlat(tj_df)  \n",
    "# tj_df.describe()\n",
    "\n",
    "# # WalMart. \n",
    "# # This one was pickled from scraping\n",
    "# with open('datasets/walmart.pkl','rb') as f:\n",
    "#     wal_df = pickle.load(f)\n",
    "\n",
    "# wal_df = pd.DataFrame(wal_df)\n",
    "\n",
    "# wal_df\n",
    "# wal_df.columns = ['long', 'lat']\n",
    "# wal_df = add_longlat(wal_df)\n",
    "# wal_df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might not need all of these imports\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon, shape, GeometryCollection\n",
    "\n",
    "\n",
    "def get_county(point, geo_df):\n",
    "    for i in range(len(geo_df)):\n",
    "        poly = geo_df.iloc[i]['geometry']\n",
    "        if poly.contains(point):\n",
    "            return us_county.iloc[i]['fips']\n",
    "        \n",
    "def get_stores_by_county(store_df, us_county):\n",
    "    found = store_df['points'].apply(get_county, args=[us_county])\n",
    "    return found.value_counts()\n",
    "\n",
    "# HIGH RESOURCE CODE >>>>>\n",
    "#wf_counts = get_stores_by_county(wf_df, us_county)  # WHOLE FOODS\n",
    "#ts_counts = get_stores_by_county(ts_df, us_county)  # TRACTOR SUPPLY STORE\n",
    "#sb_counts = get_stores_by_county(sb_df, us_county)  # STARBUCKS COFFEE \n",
    "#as_counts = get_stores_by_county(as_df, us_county)  # ACADEMY SPORTS  \n",
    "#ds_counts = get_stores_by_county(ds_df, us_county)  # DICK'S SPORTS\n",
    "#bp_counts = get_stores_by_county(bp_df, us_county)  # BASS PRO\n",
    "#dt_counts = get_stores_by_county(dt_df, us_county)  # DOLLAR TREE\n",
    "#grg_counts = get_stores_by_county(grg_df, us_county)  # GRAINGER\n",
    "#cf_counts = get_stores_by_county(cf_df, us_county)  # CHICK-FIL-A\n",
    "#cb_counts = get_stores_by_county(cb_df, us_county)  # CRACKER BARREL\n",
    "#hd_counts = get_stores_by_county(hd_df, us_county)  # HARLEY DAVIDSON\n",
    "#hm_counts = get_stores_by_county(hm_df, us_county)  # H and M\n",
    "#hl_counts = get_stores_by_county(hl_df, us_county)  # HOBBY LOBBY\n",
    "#ll_counts = get_stores_by_county(ll_df, us_county)  # LL BEAN\n",
    "#pb_counts = get_stores_by_county(pb_df, us_county)   # POTTERY BARN\n",
    "#rei_counts = get_stores_by_county(rei_df, us_county)   # REI\n",
    "#tg_counts = get_stores_by_county(tg_df, us_county)   # TARGET\n",
    "#tj_counts = get_stores_by_county(tj_df, us_county)   # TRADER JOE'S\n",
    "#wal_counts = get_stores_by_county(wal_df, us_county)   # WALMART\n",
    "\n",
    "\n",
    "\n",
    "# USE THIS TO IMPORT FROM PICKLE\n",
    "with open('datasets/store_df.pkl','rb') as f:\n",
    "    df_stores = pickle.load(f)\n",
    "\n",
    "# you can't do this copy until you align the indexes\n",
    "# df_stores.index = df2_final.index\n",
    "# df_stores['party'] = df2_final['party']\n",
    "# df_stores['blue'] = df2_final['blue']\n",
    "# df_stores.isna().sum()\n",
    "\n",
    "# df2_final.isna().sum()\n",
    "\n",
    "\n",
    "# df_stores.info()\n",
    "# df_stores.describe()\n",
    "\n",
    "# def is_blue(fips, df):\n",
    "#     try:\n",
    "#         blue = df.loc[df['fips']==fips]\n",
    "#         return blue.value\n",
    "#     except Exception as e:\n",
    "#         print(fips, e)\n",
    "\n",
    "blue = []\n",
    "\n",
    "for i in range(len(df_stores)):\n",
    "    f = df_stores.iloc[i,6]\n",
    "    b = df2_final[df2_final['fips']==str(f)]['blue']\n",
    "    blue.append(int(b))\n",
    "\n",
    "#df_stores['fips'].apply(is_blue, args=(df2_final,))  # comma has to be there UGGH\n",
    "df_stores['blue'] = blue\n",
    "# df_stores['fips'].apply(is_blue, df2_final)\n",
    "df_stores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def return_count(fips, wf_counts):\n",
    "    try:\n",
    "        return wf_counts[fips]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# df_stores['WholeFoods'] = df_stores.fips.apply(return_count, args=[wf_counts])\n",
    "# df_stores['TractorSupply'] = df_stores.fips.apply(return_count, args=[ts_counts])\n",
    "# df_stores['Starbucks'] = df_stores.fips.apply(return_count, args=[sb_counts])\n",
    "# df_stores['AcademySports'] = df_stores.fips.apply(return_count, args=[as_counts])\n",
    "# df_stores['DicksSports'] = df_stores.fips.apply(return_count, args=[ds_counts])\n",
    "# df_stores['BassPro'] = df_stores.fips.apply(return_count, args=[bp_counts])\n",
    "# df_stores['DollarTree'] = df_stores.fips.apply(return_count, args=[dt_counts])\n",
    "# df_stores['Grainger'] = df_stores.fips.apply(return_count, args=[grg_counts])\n",
    "# df_stores['ChickFila'] = df_stores.fips.apply(return_count, args=[cf_counts])\n",
    "# df_stores['CrackerBarrel'] = df_stores.fips.apply(return_count, args=[cb_counts])\n",
    "# df_stores['HarleyDavidson'] = df_stores.fips.apply(return_count, args=[hd_counts])\n",
    "# df_stores['HM'] = df_stores.fips.apply(return_count, args=[hm_counts])\n",
    "# df_stores['HobbyLobby'] = df_stores.fips.apply(return_count, args=[hl_counts])\n",
    "#df_stores['LLBean'] = df_stores.fips.apply(return_count, args=[ll_counts])\n",
    "#df_stores['PotteryBarn'] = df_stores.fips.apply(return_count, args=[pb_counts])\n",
    "#df_stores['REI'] = df_stores.fips.apply(return_count, args=[rei_counts])\n",
    "#df_stores['Target'] = df_stores.fips.apply(return_count, args=[tg_counts])\n",
    "#df_stores['TraderJoes'] = df_stores.fips.apply(return_count, args=[tj_counts])\n",
    "#df_stores['Walmart'] = df_stores.fips.apply(return_count, args=[wal_counts])\n",
    "\n",
    "#df_stores.to_pickle(\"datasets/store_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stores.to_pickle(\"datasets/store_df.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "#make_choro(df_stores1, us_county_fix, 'WholeFoods', ['lightgray', 'green'], [0, 4])\n",
    "#make_choro(df_stores2, us_county_fix, 'TractorSupply', ['lightgray', 'red'], [0, 4])\n",
    "#make_choro(df_stores5, us_county_fix, 'Starbucks', ['white', 'green'], [0, 50])\n",
    "#make_choro(df_stores5, us_county_fix, 'AcademySports', ['white', 'blue'], [0, 2])\n",
    "#make_choro(df_stores5, us_county_fix, 'DicksSports', ['white', 'orange'], [0, 2])\n",
    "#make_choro(df_stores7, us_county_fix, 'BassPro', ['lightyellow', 'red'], [0, 2])\n",
    "#make_choro(df_stores7, us_county_fix, 'DollarTree', ['white', 'green'], [0, 50])\n",
    "#make_choro(df_stores, us_county_fix, 'Grainger', ['lightgray', 'red'], [0, 3])\n",
    "#make_choro(df_stores, us_county_fix, 'ChickFila', ['lightgray', 'red'], [0, 20])\n",
    "#make_choro(df_stores, us_county_fix, 'CrackerBarrel', ['lightgray', 'gold'], [0, 4])\n",
    "#make_choro(df_stores, us_county_fix, 'HarleyDavidson', ['lightgray', 'orange'], [0, 2])\n",
    "#make_choro(df_stores, us_county_fix, 'HM', ['white', 'red'], [0, 2])\n",
    "#make_choro(df_stores, us_county_fix, 'HobbyLobby', ['white', 'orange'], [0, 3])\n",
    "#make_choro(df_stores, us_county_fix, 'LLBean', ['white', 'darkgreen'], [0, 1])\n",
    "#make_choro(df_stores, us_county_fix, 'PotteryBarn', ['white', 'blue'], [0, 1])\n",
    "#make_choro(df_stores, us_county_fix, 'REI', ['white', 'darkgreen'], [0, 1])\n",
    "#make_choro(df_stores, us_county_fix, 'Target', ['white', 'red'], [0, 10])\n",
    "#make_choro(df_stores, us_county_fix, 'TraderJoes', ['white', 'red'], [0, 5])\n",
    "make_choro(df2_final, us_county_fix, 'blue', ['red', 'blue'], [0, 1])\n",
    "#make_choro(df_stores, us_county_fix, 'blue', ['red', 'blue'], [0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete my df_store and make it final, I want to do a couple experiments\n",
    "# I want to simply add a population col\n",
    "# I would like to make a second model using stores/person for each column\n",
    "\n",
    "# Add population data\n",
    "pop_df = pd.read_csv('datasets/county_pop.csv', encoding='latin')\n",
    "#pop_df.to_csv('datasets/county_pop.csv')\n",
    "\n",
    "# pop_df.info()\n",
    "# for col in pop_df.columns:\n",
    "#     print(col)\n",
    "    \n",
    "# Keep STATE, COUNTY, CTYNAME, POPESTIMATE2018, \n",
    "pop_df = pop_df[['STATE', 'COUNTY', 'CTYNAME', 'POPESTIMATE2018']]\n",
    "pop_df.head()\n",
    "fips_list = []\n",
    "\n",
    "for i in range(len(pop_df)):\n",
    "    fips = \"{:02}{:03}\".format(pop_df.iloc[i]['STATE'], pop_df.iloc[i]['COUNTY'])\n",
    "    fips_list.append(fips)\n",
    "    \n",
    "pop_df['fips'] = pd.Series(fips_list)\n",
    "\n",
    "pop_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_county.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop(fips, pop_df):\n",
    "    try:\n",
    "        pop = pop_df[pop_df['fips']==fips]['POPESTIMATE2018'].values[0]\n",
    "        return pop\n",
    "    except:\n",
    "        print(fips)\n",
    "        return 0\n",
    "\n",
    "def get_area(fips, us_county_df):\n",
    "    try:\n",
    "        area = us_county_df[us_county_df['fips']==fips]['CENSUSAREA']\n",
    "        return float(area)\n",
    "    except Exception as e:\n",
    "        print(fips, e)\n",
    "        return 0\n",
    "    \n",
    "us_county['fips'] = us_county['fips'].astype(str)\n",
    "df_stores['population'] = df_stores.fips.apply(get_pop, args=[pop_df])\n",
    "df_stores['area'] = df_stores.fips.apply(get_area, args=[us_county])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_county.head()\n",
    "df_stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6181\n",
    "# the population of bedford city county va (fips 51515) is 6181.  Let's fill it in manually\n",
    "df_stores.loc[df_stores['fips']=='51515']\n",
    "df_stores.loc[5422,'population'] = 6181\n",
    "df_stores.loc[df_stores['fips']=='51515']  # fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good, let's pickle it and start our ML project\n",
    "#df_stores.to_pickle(\"datasets/house_df.pkl\")\n",
    "df_stores.to_pickle('datasets/all_df.pkl')\n",
    "#df_stores.to_pickle('datasets/major_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little extra to fix the df before finally sending it to pickle\n",
    "# This code was run once only to fix the dataset\n",
    "\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# # import data\n",
    "\n",
    "# with open('datasets/final_df.pkl', 'rb') as f:\n",
    "#     df = pickle.load(f)\n",
    "    \n",
    "# #had an extra column and used this to fix it\n",
    "# df = df.drop(df.columns[-1], axis=1)\n",
    "\n",
    "\n",
    "# df.party = df.party.apply(lambda x: x.title())\n",
    "# df.county = df.county.apply(lambda x: x.title())\n",
    "# # df = df.drop(columns=['totalvotes'], axis=1)\n",
    "\n",
    "# df.head()\n",
    "# #df.to_pickle('datasets/final_df.pkl')\n",
    "# df.to_pickle('datasets/house_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
